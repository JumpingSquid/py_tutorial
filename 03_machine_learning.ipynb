{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module3. Machine Learning\n",
    "Machine learning is a process to let the program itself recognize the hidden pattern behind the data. This field involves traditional statistical tools like regression and more modern approach like deep learning. We will go through some of these technique in this module based on the titanic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as usual\n",
    "# if you are working on Google Colab, please change the path to :\n",
    "# https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/titanic.csv\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the column names, there is a variable, \"Survived\", indicates whether the passenger was alive. We will try to use different models to see if we can make a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train and Evaluate\n",
    "To start training a model, we need to first split the dataset into train set and test set. The train set is the one used to train the model, and we will use the test set to evaluate the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training set and test set by 8:2\n",
    "split_n = int(0.8 * len(df))\n",
    "df_train = df.iloc[:split_n, :]\n",
    "df_test = df.iloc[split_n:, :]\n",
    "\n",
    "print(\"sample size of training set:\", len(df_train))\n",
    "print(\"sample size of test set:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select three features, Pclass, Sex, and Age. But since the gender is stored in the form of string, we need to change it to 1/0.\n",
    "Then we need to fill the nan value in the columns Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df_train.loc[:, [\"Pclass\", \"Sex\", \"Age\"]]\n",
    "train_X = train_X.replace(\"female\", 0)\n",
    "train_X = train_X.replace(\"male\", 1)\n",
    "train_X = train_X.fillna(0)\n",
    "train_y = df_train.Survived\n",
    "\n",
    "test_X = df_test.loc[:, [\"Pclass\", \"Sex\", \"Age\"]]\n",
    "test_X = test_X.replace(\"female\", 0)\n",
    "test_X = test_X.replace(\"male\", 1)\n",
    "test_X = test_X.fillna(0)\n",
    "test_y = df_test.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "We first train a linear regression model as a benchmark. When performin machine learning, it is important to choose right features. Features are the variables that the model learns from. For instance, in this case, passenger's gender, age, and class might be good features to predict the chance of survival. Although not certainly, domain knowledge can usually help you figure out what are the good features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_linear = LinearRegression().fit(train_X, train_y)\n",
    "print(\"R squared is\", clf_linear.score(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The mean squared error for the linear regression is\", mean_squared_error(clf_linear.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression is a powerful tool comparing to the linear regression when it comes to the binary case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_logistic = LogisticRegression(solver =\"lbfgs\").fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean squared error for the logistc regression is\",mean_squared_error(clf_logistic.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"logistic:\", clf_logistic.predict(test_X)[:5])\n",
    "print(\"linear:\", clf_linear.predict(test_X)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of correct guess for logistic reg:\", sum(clf_logistic.predict(test_X) == test_y))\n",
    "print(\"Accuracy: \", sum(clf_logistic.predict(test_X) == test_y)/len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_output = np.where(clf_linear.predict(test_X) > 0.5, 1, 0)\n",
    "print(\"Number of correct guess for linear reg:\", sum(binary_output == test_y))\n",
    "print(\"Accuracy: \", sum(binary_output == test_y)/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random forest is perhaps one of the most powerful prediction models in the traditional ML tools.\n",
    "![random_forest_1](https://github.com/JumpingSquid/py_tutorial/raw/master/image/rf_ilus.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=150, max_depth=3,random_state=0)\n",
    ">>> clf_rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean squared error for the random forest is\",mean_squared_error(clf_rf.predict(test_X), test_y))\n",
    "print(\"Number of correct guess for random forest:\", sum(clf_rf.predict(test_X) == test_y))\n",
    "print(\"Accuracy: \", sum(clf_rf.predict(test_X) == test_y)/len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "After using the three basic features to train the models, we may want to add more features to improve the accuracy. In practice, we can transform or combine one or more existing features to create a new feature. This is called feature engineering. In general, if we are facing a dataset with limited sample size and features, feature engineering is the key to increase the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[\"old\"] = np.where(train_X.Age > 55, 1, 0)\n",
    "train_X[\"young\"] = np.where(train_X.Age < 20, 1, 0)\n",
    "print(train_X.head())\n",
    "\n",
    "test_X[\"old\"] = np.where(test_X.Age > 55, 1, 0)\n",
    "test_X[\"young\"] = np.where(test_X.Age < 20, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[\"old_man\"] = np.where((train_X.old == 1)&(train_X.Sex==1), 1, 0)\n",
    "train_X[\"young_man\"] = np.where((train_X.young == 1)&(train_X.Sex==1), 1, 0)\n",
    "print(train_X.head())\n",
    "\n",
    "test_X[\"old_man\"] = np.where((test_X.old == 1)&(test_X.Sex==1), 1, 0)\n",
    "test_X[\"young_man\"] = np.where((test_X.young == 1)&(test_X.Sex==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_linear = LinearRegression().fit(train_X, train_y)\n",
    "clf_logistic = LogisticRegression(solver =\"lbfgs\").fit(train_X, train_y)\n",
    "clf_rf.fit(train_X, train_y)\n",
    "\n",
    "binary_output = np.where(clf_linear.predict(test_X) > 0.5, 1, 0)\n",
    "print(\"Number of correct guess for linear reg:\", sum(binary_output == test_y))\n",
    "print(\"Number of correct guess for logistic reg:\", sum(clf_logistic.predict(test_X) == test_y))\n",
    "print(\"Number of correct guess for random forest:\", sum(clf_rf.predict(test_X) == test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

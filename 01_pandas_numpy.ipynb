{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module1. Pandas and Numpy\n",
    "Pandas and numpy are the two most commonly used package for doing data analysis in Python. Pandas provides comprehensive tools for the user to manipulate the structured data, and Numpy is a package designed to handle the vector and matrix operation.\n",
    "In this module, we will focus more on Pandas, and try to use it to: 1.explore the data, 2.merge the data, and 3.clean and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Use Pandas for Exploratory Data Analysis\n",
    "Pandas is just like Excel, it is designed to handle structured data. You can use pandas to quickly produce some statistics for the data.\n",
    "This process is sometimes called exploratory data analysis(EDA). EDA is a basic but important step for doing data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data: read_csv() and read_excel()\n",
    "You can use read_csv(\"xxx.csv\") or read_excel(\"xxx.xlsx\") to read .csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# if you are working on Google Colab, please change the path to :\n",
    "# https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/titanic.csv\n",
    "df = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look: head() and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the data\n",
    "# use \"head\" to display the top n data\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use \"describe\" to show the simple stat\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and slice - part I: loc and iloc\n",
    "loc and iloc are the two major ways to get the data from the dataframe. loc takes the name or boolean mask as input, iloc take the number index (e.g. the third row with fifth column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \":\" means all rows or columns.\n",
    "<br>You can also set the starting point or the end point, like \n",
    "<br><b>\\[2:\\]</b> means from 2 to the last number\n",
    "<br><b>\\[:3\\]</b> means for the first to the second (not third!), and \n",
    "<br><b>\\[2:4\\]</b> means the second and the third.\n",
    "<br>You can also use negative number, like <b>\\[:-1\\]</b> means from the first to the last two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc use index and column name\n",
    "# loc[row index, column name]\n",
    "df.loc[:, \"Age\"] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data by row index\n",
    "df.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of course you can extract multiple index or columns by using list\n",
    "df.loc[[0,1,2], [\"Name\", \"Sex\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, you can use list to contain all the rows and columns' index\n",
    "df.iloc[[1,2,3], [1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use this way to extract the entire column\n",
    "df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and slice - part II: conditional select\n",
    "When we try to find specific columns or rows, we generally do not find iy by id but by some conditions (like SELECT and WHERE in SQL).<br>\n",
    "loc\\[\\] allows you to do that by specify the condition for the row or column in a form like:<br>\n",
    "<b>loc\\[condition for rows, condition for columns\\]</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Age < 10, [\"Name\", \"Sex\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns == \"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise One:\n",
    "Can you extract the dataframe conditional on people who stay in the third class and are female passenger?\n",
    "<br>Hint: You can use (condition 1) & (condition 2) to combine two condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ueful tools for EDA: value_counts(), groupby(), and pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number\n",
    "df.Sex.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped by\n",
    "df.groupby(by=\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table\n",
    "df.pivot_table(index=\"Sex\", columns=\"Pclass\", aggfunc=\"size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Pandas to combine the data\n",
    "In practice, it is rare to have a complete, clean, and merged data. You typically need to combine several relational dataset into one. Pandas has many tools to help you achieve this. Now let's try some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn this, we split the data into two pieces\n",
    "# Ignore this block, as this is not important at all\n",
    "df_personal = df.loc[:, [\"Name\", \"Sex\", \"Age\"]].sample(frac=1).reset_index(drop=True)\n",
    "df_ticket = df.loc[:, ['PassengerId', 'Pclass', 'Name', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].sample(frac=1).reset_index(drop=True)\n",
    "df_survival = df.loc[:, ['PassengerId', 'Survived']].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survival.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge() and concat()\n",
    "When you have multiple data, and you want to bundle them, you can use merge(). merge() basically combine the two data based on the \"key\".\n",
    "The key is usually an ID or name. Using merger(), you can choose different way to merge the data. For instance, you can decide whether to keep only the IDs that exist in both data or to keep all the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_ticket, df_survival, on='PassengerId', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the case the several data share one id, sometimes you will face the scenario that there are many dataframe with same structure but collected in different timing. To analyze the whole data, you need to use \"concatenate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = df.iloc[:400, :]\n",
    "df_new = df.iloc[400:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_old, df_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise Two:\n",
    "Please combine the three dataframe(<b>df_personal, df_survival, df_ticket</b>) into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Pandas and Numpy to clean and transform the data\n",
    "Data is not always clean. In fact, the most of your time as a data analyst will be spending on cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nan: fillna() and dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use <b>isnull()</b> to find the columns which have nan value. nan value exists when the original data has no value. It is very important to find the nan when you are doing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1: <b>fillna()</b> can fill all nan cell with a specific value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df.fillna(0)\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2: <b>dropna()</b> will drop the columns or the rows that contain nan value. It is faster but please be more cautious to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df.dropna()\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the column: using loc(), iloc(), and numpy\n",
    "If we want to change the value of a column, we need to use loc or iloc to specify the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Fare\"] = df.loc[:, \"Fare\"] * 30\n",
    "print(df.Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Fare\"] = np.mean(df.Fare)\n",
    "print(df.Fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise Three:\n",
    "Please fill the nan value in <b>Age</b> column with the mean of other passengers' age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

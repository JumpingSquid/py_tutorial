{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module1. Pandas and Numpy\n",
    "Pandas與Numpy是用Python做資料分析最常用的套件。Pandas最主要的功能在於處理表格形式的結構性資料，而Numpy則是處理多維度向量的操作。  \n",
    "這章節我們會以Pandas為主，因為大部分的資料分析工作都以結構性的資料為主。我們主要會使用Pandas來(1)探索資料、(2)合併資料與(3)清理並轉換資料。  \n",
    "Pandas and numpy are the two most commonly used package for doing data analysis in Python. Pandas provides comprehensive tools for the user to manipulate the structured data, and Numpy is a package designed to handle the vector and matrix operation.\n",
    "In this module, we will focus more on Pandas, and try to use it to: 1.explore the data, 2.merge the data, and 3.clean and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Use Pandas for Exploratory Data Analysis 用Pandas進行探索式資料分析  \n",
    "你可以把Pandas想像成Python版本的Excel，它可以用來開啟、操作、然後儲存結構性資料。當你有個csv檔或是excel檔，你都可以使用pandas來讀取資料。\n",
    "Pandas提供許多敘述性統計的功能，讓你可以快速的瞭解手上資料的長相。  \n",
    "Pandas is just like Excel, it is designed to handle structured data. You can use pandas to quickly produce some statistics for the data.\n",
    "This process is sometimes called exploratory data analysis(EDA). EDA is a basic but important step for doing data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! [Dataframe explain] (https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/image/pandas_dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the data 讀取資料: read_csv() and read_excel()\n",
    "You can use read_csv(\"xxx.csv\") or read_excel(\"xxx.xlsx\") to read .csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "# if you are working on Google Colab, please change the path to :\n",
    "# https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/titanic.csv\n",
    "df = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Take a look 擷取/敘述資料: head() and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the data\n",
    "# use \"head\" to display the top n data\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use \"describe\" to show the simple stat\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Index and slice 資料定位 - part I: loc and iloc\n",
    "有時候我們只想針對部份資料處理時，就需要使用loc或iloc來定位。  \n",
    "在Pandas的Dataframe物件旁邊，加上.loc[] 或 .iloc[,]就可以定位。逗號左邊放想要的列，右邊放想要的行。  \n",
    "loc適用於名稱或是boolean mask，而iloc則是用數字座標來選擇。  \n",
    "loc and iloc are the two major ways to get the data from the dataframe. loc takes the name or boolean mask as input, iloc take the number index (e.g. the third row with fifth column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \":\" means all rows or columns.\n",
    "<br>You can also set the starting point or the end point, like \n",
    "<br><b>\\[2:\\]</b> means from 2 to the last number\n",
    "<br><b>\\[:3\\]</b> means for the first to the second (not third!), and \n",
    "<br><b>\\[2:4\\]</b> means the second and the third.\n",
    "<br>You can also use negative number, like <b>\\[:-1\\]</b> means from the first to the last two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc use index and column name\n",
    "# loc[row index, column name]\n",
    "df.loc[:, \"Age\"] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data by row index\n",
    "df.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of course you can extract multiple index or columns by using list\n",
    "df.loc[[0,1,2], [\"Name\", \"Sex\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, you can use list to contain all the rows and columns' index\n",
    "df.iloc[[1,2,3], [1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use this way to extract the entire column\n",
    "df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and slice - part II: conditional select\n",
    "When we try to find specific columns or rows, we generally do not find iy by id but by some conditions (like SELECT and WHERE in SQL).<br>\n",
    "loc\\[\\] allows you to do that by specify the condition for the row or column in a form like:<br>\n",
    "<b>loc\\[condition for rows, condition for columns\\]</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Age < 10, [\"Name\", \"Sex\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns == \"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise One:\n",
    "Can you extract the dataframe conditional on people who stay in the third class and are female passenger?\n",
    "<br>Hint: You can use (condition 1) & (condition 2) to combine two condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ueful tools for EDA: value_counts(), groupby(), and pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number\n",
    "df.Sex.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped by\n",
    "df.groupby(by=\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table\n",
    "df.pivot_table(index=\"Sex\", columns=\"Pclass\", aggfunc=\"size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Pandas to combine the data\n",
    "In practice, it is rare to have a complete, clean, and merged data. You typically need to combine several relational dataset into one. Pandas has many tools to help you achieve this. Now let's try some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn this, we split the data into two pieces\n",
    "# Ignore this block, as this is not important at all\n",
    "df_personal = df.loc[:, [\"Name\", \"Sex\", \"Age\"]].sample(frac=1).reset_index(drop=True)\n",
    "df_ticket = df.loc[:, ['PassengerId', 'Pclass', 'Name', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']].sample(frac=1).reset_index(drop=True)\n",
    "df_survival = df.loc[:, ['PassengerId', 'Survived']].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survival.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge() and concat()\n",
    "When you have multiple data, and you want to bundle them, you can use merge(). merge() basically combine the two data based on the \"key\".\n",
    "The key is usually an ID or name. Using merger(), you can choose different way to merge the data. For instance, you can decide whether to keep only the IDs that exist in both data or to keep all the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df_ticket, df_survival, on='PassengerId', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the case the several data share one id, sometimes you will face the scenario that there are many dataframe with same structure but collected in different timing. To analyze the whole data, you need to use \"concatenate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = df.iloc[:400, :]\n",
    "df_new = df.iloc[400:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_old, df_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise Two:\n",
    "Please combine the three dataframe(<b>df_personal, df_survival, df_ticket</b>) into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Pandas and Numpy to clean and transform the data\n",
    "Data is not always clean. In fact, the most of your time as a data analyst will be spending on cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nan: fillna() and dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use <b>isnull()</b> to find the columns which have nan value. nan value exists when the original data has no value. It is very important to find the nan when you are doing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1: <b>fillna()</b> can fill all nan cell with a specific value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df.fillna(0)\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2: <b>dropna()</b> will drop the columns or the rows that contain nan value. It is faster but please be more cautious to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona = df.dropna()\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the column: using loc(), iloc(), and numpy\n",
    "If we want to change the value of a column, we need to use loc or iloc to specify the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Fare\"] = df.loc[:, \"Fare\"] * 30\n",
    "print(df.Fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"Fare\"] = np.mean(df.Fare)\n",
    "print(df.Fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise Three:\n",
    "Please fill the nan value in <b>Age</b> column with the mean of other passengers' age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

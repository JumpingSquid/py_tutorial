{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module1. Pandas and Numpy\n",
    "Pandas與Numpy是用Python做資料分析最常用的套件。Pandas最主要的功能在於處理表格形式的結構性資料，而Numpy則是處理多維度向量的操作。  \n",
    "這章節我們會以Pandas為主，因為大部分的資料分析工作都以結構性的資料為開始。我們主要會使用Pandas來(1)探索資料、(2)合併資料與(3)清理並轉換資料。\n",
    "Pandas and numpy are the two most commonly used package for doing data analysis in Python. Pandas provides comprehensive tools for the user to manipulate the structured data, and Numpy is a package designed to handle the vector and matrix operation.\n",
    "In this module, we will focus more on Pandas, and try to use it to: 1.explore the data, 2.merge the data, and 3.clean and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the package\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Use Pandas for Exploratory Data Analysis 用Pandas進行探索式資料分析  \n",
    "你可以把Pandas想像成Python版本的Excel，它可以用來開啟、操作、然後儲存結構性資料。當你有個csv檔或是excel檔，你都可以使用pandas來讀取資料。\n",
    "Pandas提供許多敘述性統計的功能，讓你可以快速的瞭解手上資料的長相。  \n",
    "Pandas is just like Excel, it is designed to handle structured data. You can use pandas to quickly produce some statistics for the data.\n",
    "This process is sometimes called exploratory data analysis(EDA). EDA is a basic but important step for doing data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataframe_explain](https://github.com/JumpingSquid/py_tutorial/blob/master/image/pandas_dataframe.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the data 讀取資料: read_csv() and read_excel()\n",
    "You can use read_csv(\"xxx.csv\") or read_excel(\"xxx.xlsx\") to read .csv or excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "# if you are working on Google Colab, please change the path to :\n",
    "# https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/titanic.csv\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/JumpingSquid/py_tutorial/master/netflix_titles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Take a look 擷取/敘述資料: head() and describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# have a look at the data\n",
    "# use \"head\" to display the top n data\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we can also use \"describe\" to show the simple stat\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Index and slice 資料定位I - part I: loc and iloc\n",
    "有時候我們只想針對部份資料處理時，就需要使用loc或iloc來定位。  \n",
    "在Pandas的Dataframe物件旁邊，加上.loc[] 或 .iloc[,]就可以定位。逗號左邊放想要的列，右邊放想要的行。  \n",
    "loc用 (1) index或columns的名稱或是 (2) boolean mask來定位，而iloc則是用數字座標來選擇。  \n",
    "loc and iloc are the two major ways to get the data from the dataframe. loc takes the name or boolean mask as input, iloc take the number index (e.g. the third row with fifth column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \":\" means all rows or columns.\n",
    "<br>You can also set the starting point or the end point, like \n",
    "<br><b>\\[2:\\]</b> means from 2 to the last number\n",
    "<br><b>\\[:3\\]</b> means for the first to the second (not third!), and \n",
    "<br><b>\\[2:4\\]</b> means the second and the third.\n",
    "<br>You can also use negative number, like <b>\\[:-1\\]</b> means from the first to the last two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# loc use index and column name\n",
    "# loc[row index, column name]\n",
    "df.loc[:, \"title\"] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# extract the data by row index\n",
    "df.loc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# of course you can extract multiple index or columns by using list\n",
    "df.loc[[0,1,2], [\"title\", \"director\", \"cast\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# iloc use the coordinate\n",
    "df.iloc[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# again, you can use list to contain all the rows and columns' index\n",
    "df.iloc[[1,2,3], [1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# you can also use this way to extract the entire column\n",
    "df.country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Index and slice 資料定位II - part II: conditional select 條件式定位\n",
    "前面講到，用loc來定位的時候，可以放進boolean mask，那甚麼是boolean mask呢? Boolean mask是一組跟行數或列數相同的長度的集合，裏頭只有True或False，代表著這一行/列是不是被指定的行/列。而這樣的特性，讓我們可以透過設定條件來找到我們想要的資料。  \n",
    "When we try to find specific columns or rows, we generally do not find iy by id but by some conditions (like SELECT and WHERE in SQL).<br>\n",
    "loc\\[\\] allows you to do that by specify the condition for the row or column in a form like:<br>\n",
    "<b>loc\\[condition for rows, condition for columns\\]</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boolmask_explain](https://github.com/JumpingSquid/py_tutorial/blob/master/image/pandas_bmask.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.release_year > 2015, [\"title\", \"director\", \"cast\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, df.columns == \"duration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example One:\n",
    "Can you extract the dataframe conditional on movies from India since 2018?\n",
    "<br>Hint: You can use (condition 1) & (condition 2) to combine two condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Other ueful tools for EDA: value_counts(), groupby(), and pivot_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# count the number\n",
    "df.country.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# grouped by\n",
    "df.groupby(by=\"country\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pivot table\n",
    "df.pivot_table(index=\"country\", columns=\"release_year\", aggfunc=\"size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Pandas to combine the data 使用pandas來合併資料\n",
    "通常在實務上，一個完整的分析會需要使用多種資料。有時候你會想要合併不同種類的資料，來增加分析的深度。\n",
    "pandas提供多種方式來讓你合併資料，在這邊我們介紹兩種不同的方式: merge() 跟 concat()。\n",
    "In practice, it is rare to have a complete, clean, and merged data.\n",
    "You typically need to combine several relational dataset into one.\n",
    "Pandas has many tools to help you achieve this. Now let's try some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# To learn this, we split the data into two pieces\n",
    "# Ignore this block, as this is not important at all\n",
    "df_staff = df.loc[:, [\"title\", \"director\", \"cast\"]].sample(frac=1).reset_index(drop=True)\n",
    "df_time = df.loc[:, ['show_id', 'title', \"country\", \"date_added\", \"release_year\"]].sample(frac=1).reset_index(drop=True)\n",
    "df_description = df.loc[:, ['show_id', 'description']].sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_staff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 merge()\n",
    "當我們有好幾份資料，而這些資料有相對應的key時(鍵值，如ID號碼、人名等)，merge()可以幫我們依照key值來合併這些資料。  \n",
    "有時候有些key可能只出現在單獨一份資料裡，你可以選擇要保留全部(缺少的資料就變成nan)，也可以選擇只保留在兩份資料都有的。  \n",
    "When you have multiple data, and you want to bundle them, you can use merge(). merge() basically combine the two data based on the \"key\".\n",
    "The key is usually an ID or name. Using merger(), you can choose different way to merge the data. For instance, you can decide whether to keep only the IDs that exist in both data or to keep all the IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boolmask_explain](https://github.com/JumpingSquid/py_tutorial/blob/master/image/pandas_merge.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(df_staff, df_description, on='title', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 concat()\n",
    "concat()用於比較簡單的合併，可以將多份資料沿著行或列去合併在一起。常見的用法有把同一份表格的新資料表跟舊資料表和在一起。  \n",
    "Besides the case the several data share one id, sometimes you will face the scenario that there are many dataframe with same structure but collected in different timing. To analyze the whole data, you need to use \"concatenate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![boolmask_explain](https://github.com/JumpingSquid/py_tutorial/blob/master/image/pandas_concat.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_old = df.iloc[:400, :]\n",
    "df_new = df.iloc[400:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([df_old, df_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise Two:\n",
    "Please combine the three dataframe(<b>df_personal, df_survival, df_ticket</b>) into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Pandas and Numpy to clean and transform the data 清理與資料轉換\n",
    "通常我們手上的資料都有一堆問題需要去清理，pandas也有提供方法來處理。  \n",
    "Data is not always perfect. In fact, the most of your time as a data analyst will be spending on cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Remove nan 移除空白值: fillna() and dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use <b>isnull()</b> to find the columns which have nan value. nan value exists when the original data has no value. It is very important to find the nan when you are doing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1: <b>fillna()</b> can fill all nan cell with a specific value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_nona = df.fillna(0)\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2: <b>dropna()</b> will drop the columns or the rows that contain nan value. It is faster but please be more cautious to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_nona = df.dropna()\n",
    "df_nona.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transform the column 資料轉換: using loc(), iloc(), and numpy\n",
    "If we want to change the value of a column, we need to use loc or iloc to specify the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"year\"] = df.loc[:, \"year\"] - 2013\n",
    "print(df.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:, \"year\"] = np.mean(df.year)\n",
    "print(df.Fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise Three:\n",
    "Please fill the nan value in <b>director</b> column with the word \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
